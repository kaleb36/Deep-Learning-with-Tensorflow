{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f98f8c15-d457-4728-8b27-fffe1fb7fbcb",
   "metadata": {},
   "source": [
    "## previous traninaed model in d22b notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ead98819-5639-43b0-87d1-0e29ae93658c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 files belonging to 2 classes.\n",
      "Found 1000 files belonging to 2 classes.\n",
      "Found 2000 files belonging to 2 classes.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "63/63 [==============================] - 42s 475ms/step - loss: 16.1557 - accuracy: 0.9020 - val_loss: 3.0563 - val_accuracy: 0.9740\n",
      "Epoch 2/50\n",
      "63/63 [==============================] - 26s 409ms/step - loss: 5.8072 - accuracy: 0.9590 - val_loss: 4.1970 - val_accuracy: 0.9690\n",
      "Epoch 3/50\n",
      "63/63 [==============================] - 26s 415ms/step - loss: 5.1584 - accuracy: 0.9540 - val_loss: 3.8796 - val_accuracy: 0.9770\n",
      "Epoch 4/50\n",
      "63/63 [==============================] - 26s 416ms/step - loss: 4.9104 - accuracy: 0.9615 - val_loss: 6.1690 - val_accuracy: 0.9710\n",
      "Epoch 5/50\n",
      "63/63 [==============================] - 27s 421ms/step - loss: 3.6770 - accuracy: 0.9755 - val_loss: 3.3626 - val_accuracy: 0.9750\n",
      "Epoch 6/50\n",
      "63/63 [==============================] - 27s 421ms/step - loss: 3.2950 - accuracy: 0.9720 - val_loss: 4.7018 - val_accuracy: 0.9730\n",
      "Epoch 7/50\n",
      "63/63 [==============================] - 27s 423ms/step - loss: 3.2661 - accuracy: 0.9755 - val_loss: 2.7981 - val_accuracy: 0.9790\n",
      "Epoch 8/50\n",
      "63/63 [==============================] - 27s 431ms/step - loss: 2.5207 - accuracy: 0.9785 - val_loss: 3.1829 - val_accuracy: 0.9790\n",
      "Epoch 9/50\n",
      "63/63 [==============================] - 27s 431ms/step - loss: 2.9277 - accuracy: 0.9745 - val_loss: 3.1912 - val_accuracy: 0.9770\n",
      "Epoch 10/50\n",
      "63/63 [==============================] - 27s 431ms/step - loss: 1.8183 - accuracy: 0.9835 - val_loss: 3.8461 - val_accuracy: 0.9750\n",
      "Epoch 11/50\n",
      "63/63 [==============================] - 27s 430ms/step - loss: 1.8647 - accuracy: 0.9810 - val_loss: 3.4661 - val_accuracy: 0.9820\n",
      "Epoch 12/50\n",
      "63/63 [==============================] - 27s 431ms/step - loss: 1.5774 - accuracy: 0.9845 - val_loss: 3.8367 - val_accuracy: 0.9770\n",
      "Epoch 13/50\n",
      "63/63 [==============================] - 27s 431ms/step - loss: 2.1135 - accuracy: 0.9835 - val_loss: 2.9571 - val_accuracy: 0.9790\n",
      "Epoch 14/50\n",
      "63/63 [==============================] - 27s 430ms/step - loss: 1.2745 - accuracy: 0.9880 - val_loss: 3.8337 - val_accuracy: 0.9750\n",
      "Epoch 15/50\n",
      "63/63 [==============================] - 27s 431ms/step - loss: 1.9885 - accuracy: 0.9800 - val_loss: 2.4346 - val_accuracy: 0.9800\n",
      "Epoch 16/50\n",
      "63/63 [==============================] - 27s 435ms/step - loss: 1.2860 - accuracy: 0.9850 - val_loss: 3.5180 - val_accuracy: 0.9830\n",
      "Epoch 17/50\n",
      "63/63 [==============================] - 27s 431ms/step - loss: 1.0615 - accuracy: 0.9915 - val_loss: 3.5187 - val_accuracy: 0.9790\n",
      "Epoch 18/50\n",
      "63/63 [==============================] - 27s 431ms/step - loss: 1.0368 - accuracy: 0.9850 - val_loss: 4.0082 - val_accuracy: 0.9830\n",
      "Epoch 19/50\n",
      "63/63 [==============================] - 27s 432ms/step - loss: 0.9936 - accuracy: 0.9865 - val_loss: 4.0111 - val_accuracy: 0.9790\n",
      "Epoch 20/50\n",
      "63/63 [==============================] - 27s 432ms/step - loss: 0.6011 - accuracy: 0.9930 - val_loss: 4.2448 - val_accuracy: 0.9770\n",
      "Epoch 21/50\n",
      "63/63 [==============================] - 27s 433ms/step - loss: 0.8972 - accuracy: 0.9885 - val_loss: 4.6034 - val_accuracy: 0.9720\n",
      "Epoch 22/50\n",
      "63/63 [==============================] - 27s 432ms/step - loss: 0.9871 - accuracy: 0.9895 - val_loss: 7.1769 - val_accuracy: 0.9660\n",
      "Epoch 23/50\n",
      "63/63 [==============================] - 27s 433ms/step - loss: 1.0432 - accuracy: 0.9885 - val_loss: 3.1744 - val_accuracy: 0.9840\n",
      "Epoch 24/50\n",
      "63/63 [==============================] - 28s 436ms/step - loss: 0.7174 - accuracy: 0.9915 - val_loss: 2.5346 - val_accuracy: 0.9810\n",
      "Epoch 25/50\n",
      "63/63 [==============================] - 28s 436ms/step - loss: 0.6147 - accuracy: 0.9900 - val_loss: 3.3601 - val_accuracy: 0.9820\n",
      "Epoch 26/50\n",
      "63/63 [==============================] - 27s 436ms/step - loss: 0.6070 - accuracy: 0.9935 - val_loss: 4.0958 - val_accuracy: 0.9790\n",
      "Epoch 27/50\n",
      "63/63 [==============================] - 28s 436ms/step - loss: 1.1443 - accuracy: 0.9890 - val_loss: 3.2083 - val_accuracy: 0.9800\n",
      "Epoch 28/50\n",
      "63/63 [==============================] - 27s 436ms/step - loss: 0.5058 - accuracy: 0.9925 - val_loss: 4.1170 - val_accuracy: 0.9780\n",
      "Epoch 29/50\n",
      "63/63 [==============================] - 27s 436ms/step - loss: 0.8481 - accuracy: 0.9860 - val_loss: 4.4975 - val_accuracy: 0.9770\n",
      "Epoch 30/50\n",
      "63/63 [==============================] - 28s 437ms/step - loss: 0.4810 - accuracy: 0.9945 - val_loss: 4.8676 - val_accuracy: 0.9770\n",
      "Epoch 31/50\n",
      "63/63 [==============================] - 28s 438ms/step - loss: 0.6791 - accuracy: 0.9905 - val_loss: 4.2259 - val_accuracy: 0.9790\n",
      "Epoch 32/50\n",
      "63/63 [==============================] - 28s 439ms/step - loss: 0.4947 - accuracy: 0.9930 - val_loss: 4.1357 - val_accuracy: 0.9810\n",
      "Epoch 33/50\n",
      "63/63 [==============================] - 28s 438ms/step - loss: 0.5288 - accuracy: 0.9905 - val_loss: 5.0583 - val_accuracy: 0.9740\n",
      "Epoch 34/50\n",
      "63/63 [==============================] - 28s 438ms/step - loss: 0.6564 - accuracy: 0.9920 - val_loss: 3.0624 - val_accuracy: 0.9850\n",
      "Epoch 35/50\n",
      "63/63 [==============================] - 28s 438ms/step - loss: 0.4871 - accuracy: 0.9920 - val_loss: 2.7350 - val_accuracy: 0.9810\n",
      "Epoch 36/50\n",
      "63/63 [==============================] - 28s 439ms/step - loss: 0.3724 - accuracy: 0.9945 - val_loss: 4.3983 - val_accuracy: 0.9780\n",
      "Epoch 37/50\n",
      "63/63 [==============================] - 28s 439ms/step - loss: 0.8817 - accuracy: 0.9895 - val_loss: 2.6830 - val_accuracy: 0.9860\n",
      "Epoch 38/50\n",
      "63/63 [==============================] - 28s 440ms/step - loss: 0.4501 - accuracy: 0.9915 - val_loss: 2.7236 - val_accuracy: 0.9850\n",
      "Epoch 39/50\n",
      "63/63 [==============================] - 28s 440ms/step - loss: 0.9018 - accuracy: 0.9915 - val_loss: 3.0991 - val_accuracy: 0.9840\n",
      "Epoch 40/50\n",
      "63/63 [==============================] - 28s 441ms/step - loss: 0.5038 - accuracy: 0.9935 - val_loss: 2.7763 - val_accuracy: 0.9800\n",
      "Epoch 41/50\n",
      "63/63 [==============================] - 28s 440ms/step - loss: 0.5726 - accuracy: 0.9940 - val_loss: 3.5071 - val_accuracy: 0.9790\n",
      "Epoch 42/50\n",
      "63/63 [==============================] - 28s 441ms/step - loss: 0.3487 - accuracy: 0.9970 - val_loss: 3.5780 - val_accuracy: 0.9810\n",
      "Epoch 43/50\n",
      "63/63 [==============================] - 28s 441ms/step - loss: 0.6292 - accuracy: 0.9915 - val_loss: 4.4033 - val_accuracy: 0.9810\n",
      "Epoch 44/50\n",
      "63/63 [==============================] - 28s 441ms/step - loss: 0.3092 - accuracy: 0.9965 - val_loss: 4.4048 - val_accuracy: 0.9780\n",
      "Epoch 45/50\n",
      "63/63 [==============================] - 28s 443ms/step - loss: 0.7613 - accuracy: 0.9910 - val_loss: 3.9580 - val_accuracy: 0.9770\n",
      "Epoch 46/50\n",
      "63/63 [==============================] - 28s 441ms/step - loss: 0.6380 - accuracy: 0.9905 - val_loss: 3.6306 - val_accuracy: 0.9780\n",
      "Epoch 47/50\n",
      "63/63 [==============================] - 28s 442ms/step - loss: 0.3164 - accuracy: 0.9960 - val_loss: 3.3647 - val_accuracy: 0.9790\n",
      "Epoch 48/50\n",
      "63/63 [==============================] - 28s 443ms/step - loss: 0.2745 - accuracy: 0.9950 - val_loss: 3.4059 - val_accuracy: 0.9810\n",
      "Epoch 49/50\n",
      "63/63 [==============================] - 28s 442ms/step - loss: 0.6003 - accuracy: 0.9910 - val_loss: 3.6279 - val_accuracy: 0.9770\n",
      "Epoch 50/50\n",
      "63/63 [==============================] - 28s 442ms/step - loss: 0.6209 - accuracy: 0.9910 - val_loss: 3.7559 - val_accuracy: 0.9800\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "train_dataset = image_dataset_from_directory(\n",
    "    \"cats_vs_dogs_small\\\\train\",\n",
    "    image_size=(180,180),\n",
    "    batch_size=32)\n",
    "validation_dataset = image_dataset_from_directory(\n",
    "    \"cats_vs_dogs_small\\\\validation\",\n",
    "    image_size=(180,180),\n",
    "    batch_size=32)\n",
    "test_dataset = image_dataset_from_directory(\n",
    "    \"cats_vs_dogs_small\\\\test\",\n",
    "    image_size=(180,180),\n",
    "    batch_size=32)\n",
    "\n",
    "#convolution base\n",
    "conv_base = keras.applications.vgg16.VGG16(\n",
    "    weights=\"imagenet\",\n",
    "    include_top=False)\n",
    "#setting convolution base to False to freeze the weights during training\n",
    "conv_base.trainable = False \n",
    "\n",
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.1),\n",
    "    layers.RandomZoom(0.2),\n",
    "    ]\n",
    ")\n",
    "\n",
    "#our model\n",
    "inputs = keras.Input(shape=(180, 180, 3))\n",
    "x = data_augmentation(inputs) #applying data augmentation\n",
    "x = keras.applications.vgg16.preprocess_input(x) #applying value scaling\n",
    "x = conv_base(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(256)(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "             optimizer=\"rmsprop\",\n",
    "             metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=50,\n",
    "    validation_data=validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c27f033-03de-4a55-b88e-8217e2c74adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "63/63 [==============================] - 36s 469ms/step - loss: 3.6409 - accuracy: 0.9735 - val_loss: 3.3292 - val_accuracy: 0.9800\n",
      "Epoch 2/30\n",
      "63/63 [==============================] - 28s 443ms/step - loss: 1.8272 - accuracy: 0.9770 - val_loss: 2.5546 - val_accuracy: 0.9830\n",
      "Epoch 3/30\n",
      "63/63 [==============================] - 28s 446ms/step - loss: 1.7530 - accuracy: 0.9740 - val_loss: 3.0309 - val_accuracy: 0.9810\n",
      "Epoch 4/30\n",
      "63/63 [==============================] - 28s 449ms/step - loss: 1.0800 - accuracy: 0.9830 - val_loss: 2.9912 - val_accuracy: 0.9790\n",
      "Epoch 5/30\n",
      "63/63 [==============================] - 29s 453ms/step - loss: 1.0161 - accuracy: 0.9865 - val_loss: 2.3937 - val_accuracy: 0.9800\n",
      "Epoch 6/30\n",
      "63/63 [==============================] - 29s 452ms/step - loss: 0.6099 - accuracy: 0.9910 - val_loss: 2.4818 - val_accuracy: 0.9820\n",
      "Epoch 7/30\n",
      "63/63 [==============================] - 29s 455ms/step - loss: 0.6613 - accuracy: 0.9860 - val_loss: 2.6330 - val_accuracy: 0.9820\n",
      "Epoch 8/30\n",
      "63/63 [==============================] - 29s 457ms/step - loss: 0.4755 - accuracy: 0.9935 - val_loss: 2.5601 - val_accuracy: 0.9800\n",
      "Epoch 9/30\n",
      "63/63 [==============================] - 30s 480ms/step - loss: 0.5063 - accuracy: 0.9885 - val_loss: 2.6244 - val_accuracy: 0.9820\n",
      "Epoch 10/30\n",
      "63/63 [==============================] - 31s 491ms/step - loss: 0.3869 - accuracy: 0.9925 - val_loss: 2.9943 - val_accuracy: 0.9830\n",
      "Epoch 11/30\n",
      "63/63 [==============================] - 31s 483ms/step - loss: 0.5782 - accuracy: 0.9905 - val_loss: 2.7201 - val_accuracy: 0.9790\n",
      "Epoch 12/30\n",
      "63/63 [==============================] - 29s 460ms/step - loss: 0.4607 - accuracy: 0.9905 - val_loss: 2.6654 - val_accuracy: 0.9790\n",
      "Epoch 13/30\n",
      "63/63 [==============================] - 29s 461ms/step - loss: 0.5673 - accuracy: 0.9900 - val_loss: 2.8783 - val_accuracy: 0.9780\n",
      "Epoch 14/30\n",
      "63/63 [==============================] - 29s 461ms/step - loss: 0.4203 - accuracy: 0.9900 - val_loss: 2.8306 - val_accuracy: 0.9780\n",
      "Epoch 15/30\n",
      "63/63 [==============================] - 29s 461ms/step - loss: 0.3693 - accuracy: 0.9945 - val_loss: 2.5408 - val_accuracy: 0.9800\n",
      "Epoch 16/30\n",
      "63/63 [==============================] - 29s 464ms/step - loss: 0.2392 - accuracy: 0.9930 - val_loss: 2.6879 - val_accuracy: 0.9800\n",
      "Epoch 17/30\n",
      "63/63 [==============================] - 29s 462ms/step - loss: 0.2091 - accuracy: 0.9940 - val_loss: 2.6376 - val_accuracy: 0.9800\n",
      "Epoch 18/30\n",
      "63/63 [==============================] - 29s 464ms/step - loss: 0.1980 - accuracy: 0.9950 - val_loss: 2.8878 - val_accuracy: 0.9780\n",
      "Epoch 19/30\n",
      "63/63 [==============================] - 29s 462ms/step - loss: 0.1367 - accuracy: 0.9955 - val_loss: 2.5109 - val_accuracy: 0.9810\n",
      "Epoch 20/30\n",
      "63/63 [==============================] - 29s 467ms/step - loss: 0.0875 - accuracy: 0.9970 - val_loss: 2.2214 - val_accuracy: 0.9830\n",
      "Epoch 21/30\n",
      "63/63 [==============================] - 29s 465ms/step - loss: 0.0994 - accuracy: 0.9965 - val_loss: 2.7923 - val_accuracy: 0.9760\n",
      "Epoch 22/30\n",
      "63/63 [==============================] - 29s 465ms/step - loss: 0.3418 - accuracy: 0.9930 - val_loss: 2.2909 - val_accuracy: 0.9830\n",
      "Epoch 23/30\n",
      "63/63 [==============================] - 29s 466ms/step - loss: 0.2878 - accuracy: 0.9945 - val_loss: 1.9772 - val_accuracy: 0.9830\n",
      "Epoch 24/30\n",
      "63/63 [==============================] - 29s 463ms/step - loss: 0.1365 - accuracy: 0.9965 - val_loss: 2.0261 - val_accuracy: 0.9810\n",
      "Epoch 25/30\n",
      "63/63 [==============================] - 29s 464ms/step - loss: 0.3078 - accuracy: 0.9960 - val_loss: 2.2439 - val_accuracy: 0.9800\n",
      "Epoch 26/30\n",
      "63/63 [==============================] - 29s 463ms/step - loss: 0.1326 - accuracy: 0.9970 - val_loss: 2.2780 - val_accuracy: 0.9770\n",
      "Epoch 27/30\n",
      "63/63 [==============================] - 29s 465ms/step - loss: 0.0645 - accuracy: 0.9980 - val_loss: 2.5073 - val_accuracy: 0.9780\n",
      "Epoch 28/30\n",
      "63/63 [==============================] - 29s 466ms/step - loss: 0.0715 - accuracy: 0.9960 - val_loss: 2.7647 - val_accuracy: 0.9830\n",
      "Epoch 29/30\n",
      "63/63 [==============================] - 30s 468ms/step - loss: 0.0711 - accuracy: 0.9975 - val_loss: 2.4598 - val_accuracy: 0.9820\n",
      "Epoch 30/30\n",
      "63/63 [==============================] - 29s 464ms/step - loss: 0.0427 - accuracy: 0.9975 - val_loss: 2.4315 - val_accuracy: 0.9830\n"
     ]
    }
   ],
   "source": [
    "#Freezing all layers until the fourth from the list\n",
    "conv_base.trainable = True\n",
    "for layer in conv_base.layers[:-4]:\n",
    "    layer.trainable = False\n",
    "\n",
    "#fine-tuning the model\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "             optimizer=keras.optimizers.RMSprop(learning_rate=1e-5),\n",
    "             metrics=[\"accuracy\"])\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=\"fine_tuning.keras\",\n",
    "        save_best_only=True,\n",
    "        monitor=\"val_loss\")\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=30,\n",
    "    validation_data=validation_dataset,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f81971aa-936e-4e64-b79e-c53b7532d057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 12s 174ms/step - loss: 2.5958 - accuracy: 0.9765\n",
      "Test accuracy: 0.976\n"
     ]
    }
   ],
   "source": [
    "test_model = keras.models.load_model(\"fine_tuning.keras\")\n",
    "test_loss, test_acc = test_model.evaluate(test_dataset)\n",
    "print(f\"Test accuracy: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a494d96e-a622-4116-b8dd-7073a330a68b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaltf",
   "language": "python",
   "name": "kaltf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
